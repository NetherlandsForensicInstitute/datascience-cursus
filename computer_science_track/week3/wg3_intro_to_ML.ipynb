{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7846aab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "Welcome to the practical on Machine Learning. \n",
    "Today we are going to start by loading and preprocessing a new dataset. Then we'll see how feature extraction can be done and how these features can be used to train a model.   \n",
    "\n",
    "The dataset we will be working with is a collection of WhatsApp messages that has been collected by the Radboud University. You can find more information on the dataset here: https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:112987.\n",
    "\n",
    "This dataset was originally in Dutch, but we have translated it automatically into English. You might notice some strange mistakes that are caused by the translation.\n",
    "\n",
    "This dataset has been (partially) annotated by the NFI in order to tag *meetings*, i.e., messages which indicate that meetings were being planned, or which refer to a meeting that had already been planned. Our task is to train a model which can (correctly) classify messages. That is, does a message make reference to a meeting? \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e8fac-33bf-4323-b323-d824b0fac576",
   "metadata": {},
   "source": [
    "## Loading our data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load our dataset to see what it looks like\n",
    "import pandas as pd\n",
    "\n",
    "whatsapp_file = 'wg3_intro_to_ml_data_balanced_translated.csv'\n",
    "df = pd.read_csv(whatsapp_file)\n",
    "\n",
    "display(df.head())\n",
    "print('Total number of entries:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10374418-6a2d-4aa4-be5f-f937dfee5cbe",
   "metadata": {},
   "source": [
    "You can still see the original text (in Dutch) in column `dutch_text` and the new translated English text in the `text` column. Let's remove the Dutch text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f902b-2115-4814-8e83-5e0a98bf1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we simply drop (or leave out) the column with the Dutch text\n",
    "df = df.drop(labels = ['dutch_text'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733d3b0-8363-417d-9289-c5a2e29d25ef",
   "metadata": {},
   "source": [
    "As you can see above, our data consists of 3 columns called 'id', 'text' and 'label' and it contains 608 entries.  The labels of this data are 1 and 0: 1 for when a meeting is discussed, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how many labels we have of each\n",
    "# we can do this using Counter\n",
    "from collections import Counter\n",
    "Counter(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb15085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's investigate the text entries of our dataset a little bit closer\n",
    "# setting the max column width\n",
    "pd.options.display.max_colwidth = 90 # you can adjust this number so that the text is legible in your screen\n",
    "\n",
    "# display the first 15 entries of the text column\n",
    "df[\"text\"][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9256708",
   "metadata": {},
   "source": [
    "As you can see there is a lot going on here. The messages contain capital letters, interpunction and it seems that someone has replaced certain words with [REMOVED].\n",
    "\n",
    "In order to classify whether people are discussing to meet up, we do not need these things. In fact, they could make our life a lot harder (you will find out why later in this notebook). So let's clean up a bunch of these things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d7c86-a8ac-4eb5-9846-eaedfe413b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new column called \"clean_text\" that contains the cleaned data\n",
    "# use .lower() to lowercase any string\n",
    "df[\"clean_text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f1367-7569-460a-96b2-a774bb622a5c",
   "metadata": {},
   "source": [
    "Now we have lowered the text we still want to remove punctuation and [REMOVED] from our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2aced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import the re package which allows us to search for sequences in our text that match a predefined pattern.\n",
    "import re\n",
    "\n",
    "# re allows us to substitute any given string with something else, in this case an empty string, effectively deleting it\n",
    "def delete_substring(pattern, string): \n",
    "    return re.sub(pattern, '', str(string))\n",
    "     \n",
    "# we use df.apply() to call the function delete_substring on each row, which takes as input the substring to \n",
    "# delete ([removed]), and the cleaned text\n",
    "df[\"clean_text\"] = df.apply(lambda x: delete_substring(r'\\[removed]', x[\"clean_text\"]), axis = 1)\n",
    "\n",
    "# below we substitute anything that is not a letter, space or digit (in practice that is punctuation) with an \n",
    "# empty string\n",
    "df[\"clean_text\"] = df.apply(lambda x: delete_substring(r'[^\\w\\s]', x[\"clean_text\"]), axis = 1)\n",
    "\n",
    "# display the first 15 entries of column clean_text\n",
    "df[\"clean_text\"][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb07dbd",
   "metadata": {},
   "source": [
    "## Feature creation\n",
    "Machine learning algorithms take numbers as input. This can be a bit counter-intuitive since we work with human-language data. \n",
    "\n",
    "The simplest method of creating input (called *features*) from language, is to count whether/how often certain words occur in our sample. In our case, words of interest may be *where* and *when*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's create some features:\n",
    "\n",
    "relevant_words = [\n",
    "        \"where\",\n",
    "        \"when\" \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb806da9",
   "metadata": {},
   "source": [
    "### Exercise: complete the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbe136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn! \n",
    "# write a function below which returns 1 if a certain word is in a string, and 0 if that word is not in the string\n",
    "\n",
    "def count(word, string):\n",
    "    ### YOUR CODE HERE! ###\n",
    "    pass\n",
    "    \n",
    "# you can test if it works by uncommenting the code below:\n",
    "# word = \"hello\"\n",
    "# string = \"hello world!\"\n",
    "# count(word, string) # should return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a66848-0637-4688-86a9-eb2aca4fb616",
   "metadata": {},
   "source": [
    "### Using occurrence of words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below looks quite complicated, but don't get discouraged, I'll explain what happens:\n",
    "\n",
    "# we loop through our list of relevant words, which contains, in this case, the words \"where\" and \"when\"\n",
    "for word in relevant_words:\n",
    "    # we create a new column in the dataframe for each relevant word, where the column name is the word itself (df[word])\n",
    "    # we use df.apply() to call the function count() on each row, which takes as input the relevant word, and the cleaned text\n",
    "    df[word] = df.apply(lambda x: count(word, x[\"clean_text\"]), axis=1)\n",
    "\n",
    "# the result can be seen below, for each relevant word we have a column in the data, that contains a 1 \n",
    "# if that particular word occurs in the text, and 0 if it does not\n",
    "df[[\"clean_text\", \"label\", \"where\", \"when\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ac613-2618-4ac0-a8e0-16f170016db4",
   "metadata": {},
   "source": [
    "As you can see above, the words we've chosen are actually not present in messages 2 and 3, which are messages about \n",
    "meetings (i.e., column label is equal to 1). This indicates that this choice of features may not be adequate for training a model, but we will work with it for the purpose of illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02f798-0c9d-4413-9c3d-d442d0b74686",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "There is one more thing we need to do in order to start training: divide our data in train and test sets. This is an import step in order to avoid *overfitting*. That is, a model that fits its training data far too well and, therefore, fails to fit additional data. Such a model cannot reliably be applied to classify unseen data. Keeping a small part of the data separately allows us to check whether our model is overfitting on the training data or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luckily, sklearn has a function that does this for us\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we use 20% of our data for testing, as indicated in the variable test_size\n",
    "# the random_state variable controls the shuffling of the data before the split is applied\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=4)\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "\n",
    "df_train[[\"clean_text\", \n",
    "          \"label\",\n",
    "          \"where\", \n",
    "          \"when\" \n",
    "        ]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc47c0b0-6a9f-426a-ad9b-bd35d14994cc",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "\n",
    "As you can see from the indices of the data, the dataset has been shuffled before dividing the dataset into train and test sets. Before we train our model, let's have a look at our data and the features we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cb564-95f3-4cf9-8a66-811a954ef375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this is just some code to make sure not all dots overlap, so we can see how many points are where\n",
    "df_train[\"jittered_where\"] = df[\"where\"] - 0.3* np.random.rand(len(df[\"where\"])) -0.05\n",
    "df_train[\"jittered_when\"] = df[\"when\"] - 0.3* np.random.rand(len(df[\"when\"])) -0.05\n",
    "\n",
    "# let's plot our data!\n",
    "ax2 = df_train.plot(kind='scatter',\n",
    "                     x=\"jittered_where\",\n",
    "                     y=\"jittered_when\",\n",
    "                     c=\"label\",\n",
    "                     colormap=\"winter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1bfc0-4195-4996-b598-23e0a25a8a05",
   "metadata": {},
   "source": [
    "Above you see a scatterplot, on the x-axis is whether the word \"where\" occurs (1) or not (0), and on the y-axis you can see if the word \"when\" occurs (1) or not (0). If the messages discuss a meeting, they are green; otherwise, they are coloured blue. \n",
    "\n",
    "As you may have noticed, our features do not allow for a perfect split between the samples that discuss a meeting and those that do not. So, most likely, our machine learning model won't be able to perfectly distinguish them either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d689df0",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "Let's see how a model can be trained using the features above. We use scikit-learn again (also known as sklearn), a common package that includes not only a function to split datasets but many machine learning models as well. See also [*the website*](https://scikit-learn.org/stable/).\n",
    "\n",
    "We will start with one of the most basic models in machine learning: *logistic regression*. [*Logistic regression*](https://en.wikipedia.org/wiki/Logistic_regression) models the probability of an event based on the input variables. In this case, it models the probability for a message being a meeting, given features that we will define below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af922d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we initiate the model so it's ready to use\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0d687",
   "metadata": {},
   "source": [
    "Now that we have initiated the model, we can fit the model to our data (the data being the features we have derived and the associated labels). 'Fitting' simply means finding the weights in our logistic regression equations that best 'fit' with our data, i.e. which most often give the right result (NB technically, we are optimising a loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "     df_train[[ \n",
    "         \"where\", # 'where'\n",
    "         \"when\" # 'when'\n",
    "        ]], \n",
    "    df_train['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6954d6-982f-464c-bef5-337c5815dd72",
   "metadata": {},
   "source": [
    "And that's it! We now have a trained model, that can be used to perform authorship predictions on new texts. Congratulations, you have just done machine learning!\n",
    "\n",
    "Now let's have a quick look at how well our model does. Keep in mind that we look at the performance on the test data (df_test), the performance on the training data (df_train) is probably higher as the model has already seen that data. Like doing an exam when you've already discussed the questions in class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc86781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_predictions = model.predict(\n",
    "    df_test[[\"where\", \"when\"]]) \n",
    "\n",
    "accuracy_score(df_test['label'], model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9262d",
   "metadata": {},
   "source": [
    "An accuracy of 0.52 is not great. And, why is that? Because it only performs slightly better than the toss of a coin. In fact, we might as well have assigned our samples randomly to one of the classes.\n",
    "\n",
    "Can you think of ways to improve our model? For instance, other words that would be relevant for our case?  \n",
    "\n",
    "### Exercise: Improve the performance of our model by adding a few extra words to our list of relevant_words. \n",
    "\n",
    "The code that you need in order to do this is below. You can also use the code block directly below to inspect some messages with a positive label. This is optional but can be very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563047f3-8361-4334-8a43-85f60341e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) If you want, you can use this block to write some code to have a look specifically at messages about meetings\n",
    "\n",
    "def show_messages_with_label(label): \n",
    "    #YOUR CODE FOR SELECTING DATA WITH A CERTAIN LABEL\n",
    "    pass\n",
    "\n",
    "# Uncomment the code below to check if your function works. It should return True. \n",
    "# We are checking if the length (len()) of the data is equal to the sum of all the labels which should\n",
    "# be the case if all the messages have the label 1\n",
    "\n",
    "# df_with_one_label = show_messages_with_label(1)\n",
    "# print(sum(df_with_one_label['label']) == len(df_with_one_label)) \n",
    "      \n",
    "# Uncomment the code below to use your function to select 10 messages with the positive (1) label\n",
    "\n",
    "# show_messages_with_label(label=1)['clean_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to add some extra words to the relevant_words to improve performance. \n",
    "# Make sure you get the indentation right.\n",
    "\n",
    "# Possible words that help the model are: \n",
    "\n",
    "relevant_words = [\n",
    "    \"where\", \n",
    "    \"when\",\n",
    "]\n",
    "\n",
    "for word in relevant_words:\n",
    "    df[word] = df.apply(lambda x: count(word, x[\"clean_text\"]), axis=1)\n",
    "\n",
    "    \n",
    "# uncomment the print statements if you wanna check what your data looks like now\n",
    "# print(df.head())\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=4)\n",
    "\n",
    "# display(df_train[relevant_words])\n",
    "\n",
    "model.fit(\n",
    "    df_train[relevant_words], \n",
    "    df_train['label']\n",
    ")\n",
    "\n",
    "model_predictions = model.predict(\n",
    "    df_test[relevant_words]) \n",
    "\n",
    "print(accuracy_score(df_test['label'], model_predictions))\n",
    "\n",
    "if accuracy_score(df_test['label'], model_predictions) > 0.5163934426229508:\n",
    "                  print(\"Congratulations, you improved the accuracy of the model!\")\n",
    "else:\n",
    "    print(\"Your model did not outperform our previous model, try again\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5469e",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "There may be a lot more words that indicate meetings being discussed, that we did not think of yet. One way of not having to think of all these words is by using a count vectorizer. This function creates a matrix (= list of lists) that counts how often any word that occurs at least once in the entire dataset (= the vocabulary) occurs in each sample. \n",
    "\n",
    "An example from the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \n",
    "\n",
    "```\n",
    "corpus = [\n",
    "     'This is the first document.'\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    "       \n",
    "print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    " ```\n",
    "As you can see, 'and' is the first column of the array above. It occurs only in the third sample, so there is a 1 in that row, and a 0 in the others. The second word is 'document', it occurs in the first and fourth sample, and it occurs even twice in the second sample. Therefore there is a one in the first and fourth row of the second column, and a two in the second row of the second column.\n",
    "\n",
    "### Question: Can you think of a reason why we lowercased our data and removed all punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea064ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "features_train = cv.fit_transform(df_train[\"clean_text\"])\n",
    "features_test = cv.transform(df_test[\"clean_text\"])\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed284e27",
   "metadata": {},
   "source": [
    "As you can see above, our training data now has 486 (= size of the subset) rows and 1045 (= vocabulary size) columns.\n",
    "\n",
    "Mind that we call fit_transform() on the training data and transform() on the test data. This means that we create features based on the training data, and then check if those features occur in the test data. If you were to call fit_transform() twice you would just have two completely different datasets based on completely different vocabularies (try it!).\n",
    "\n",
    "This means that any words in the testset that are not in the vocabulary of the trainingset are disregarded. This may sound inconvenient, but you cannot train on what you don't have. Yet another example of why you need much and diverse training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we print a part of the feature names, we see that they contain words from our data\n",
    "\n",
    "print(cv.get_feature_names_out()[:15])\n",
    "\n",
    "# you can see that the features look like a big list of list of zeroes and ones \n",
    "# most of them are zeroes because only very few words from the vocabulary actually occur in each sample\n",
    "\n",
    "print(features_train.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8be01-0ba4-4d58-8fe6-6a76bed3ff43",
   "metadata": {},
   "source": [
    "Now that we have these features, all we have to do is put it in the model we have defined before. We do not need to specify all of our features like we did before, we stored them in a variable called 'features' which we can input completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features_train, df_train[\"label\"])\n",
    "model_predictions = model.predict(features_test)\n",
    "print(accuracy_score(df_test['label'], model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0987b-4c60-4ee6-a4ff-58a62b9facc0",
   "metadata": {},
   "source": [
    "And, look! we significantly improved the performance of our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbc4e9",
   "metadata": {},
   "source": [
    "### Further exercises:\n",
    "\n",
    "1. Last time you learned about word clouds. See if you can make a word cloud for the messages that are about meetings (with label 1). You will need to run the cell below to install and import the functions you will need. You can look here for help: \n",
    "https://www.python-graph-gallery.com/wordcloud/\n",
    "\n",
    "You will also need to join all the texts into one long string using the following: \n",
    "\n",
    "```\n",
    "joined_text = ' '.join(texts_to_join)\n",
    "```\n",
    "\n",
    "You should do this after you have selected all the texts with a positive label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402752f0-5888-4eea-9090-7ae78fddecf7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160049f-759a-4104-88e3-dffc5e250c92",
   "metadata": {},
   "source": [
    "2. Where we split the data in a train and a test set, we set a *random_state* and we've mentioned that *random_state* controls the shuffling of the data \n",
    "\n",
    "```\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "```\n",
    " \n",
    "In detail, a random state basically allows you to control the randomness train_test_split() uses when you run it. If you don't set a random_state, your train and test set will be different every time you run this function (even if you run it on the same data). Remove the random state and run the cell a few times, see how the top of the trainingset changes every time. If you would run the whole model every time train_test_split had a new configuration, you would see that the performance of the model would be slightly different as well. This makes sense because the split between train and test are different. This is why we often use cross-validation when we report the performance of our models in academic papers: we run the model multiple times with different train/test splits, to see how our model performs on average.\n",
    "\n",
    "3. Before we talked about generating features for our dataset, we have converted the messages to lower case and removed punctuations. This is called *normalisation*. Here, we will discuss some basic steps needed for Text normalization. Other normalisation steps can be removing all numbers or, if they are essential, converting them to textual form; removing extra white spaces; and, removing stop words (i.e., words which can be filtered out without loss of information such as \"the\", \"and\", \"of\", etc.). Try implementing them!\n",
    "\n",
    "For stop words you could have a look here: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "4. Now that you have finished the notebook, go back section \"Training a model\" and experiment with different models. You can replace logistic regression with other models, such as a Naive Bayes (from sklearn.naive_bayes import GaussianNB) or a Support Vector Machine (from sklearn.svm import SVC). These models are most frequently used with language data, but you can also check the documentation (https://scikit-learn.org/stable/supervised_learning.html) to try out other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0804dfa78a9ed2",
   "metadata": {},
   "source": [
    "# Kaggle Challenge\n",
    "\n",
    "Do you want to put your data science skills to the test? See if you can implement your own model that predicts poisonous mushrooms in the Kaggle challenge: https://www.kaggle.com/t/3fb3213893214f28825b0f8848e471c9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscursus",
   "language": "python",
   "name": "dscursus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
