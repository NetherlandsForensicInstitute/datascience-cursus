{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927a219b",
   "metadata": {},
   "source": [
    "# Doing ML - the right way\n",
    "\n",
    "Welcome back! \n",
    "\n",
    "In the previous tutorial, we saw how to load and clean our data. We created features to translate text into numbers and we trained a machine learning model using those features. In this tutorial, we will discuss about evaluation metrics, imbalanced data, and why it is important to split your dataset into a training and a test set. Finally, we will briefly discuss how to recognise whether we are dealing with high bias or variance, and finally what happens when there is a mismatch between the training and test set. \n",
    "\n",
    "Note that throughout this tutorial, you will find that some code cells contain the expression `None`. In this case, it is expected from you to delete this `None` and place your code there (it is always expected that one `None` corresponds to one line of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the libraries you need for this tutorial\n",
    "# please run this cell before you proceed further\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c5a2a",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are going to use the same dataset as last time: a collection of (Dutch) WhatsApp messages collected by Radboud University. It was (partially) annotated by the NFI for meetings, i.e. messages that indicated meetings were being planned, or that referred to a meeting that had already been planned. You can find more information on the dataset here: https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:112987.\n",
    "\n",
    "Last time we worked with the translated messages, but this time we will work with the original Dutch messages. No worries if you don't speak Dutch. You will not be selecting features manuallly this time.\n",
    "\n",
    "As before, our dataset contains three columns called 'id' (= identifier of the message), 'text' ( = the actual message), and 'label' (1 for when a meeting is discussed, and 0 if there is not). There is only one difference: the dataset used in this tutorial has more data points. Let's continue with loading our data and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using what you learned in the previous tutorials, load the file \"wg4_doing_ML_the_right_way.csv\" \n",
    "# as a pandas dataframe called 'df' and print its first 10 entries\n",
    "whatsapp_file = None  # name of the file\n",
    "df = None  # load file\n",
    "None  # print first 10 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98962d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how many entries in total and per label we have\n",
    "print('Number of entries = ', df.shape[0])\n",
    "label_counts = Counter(df['label'])\n",
    "print('Number of entries with label 0 (no meeting) =', label_counts[0])\n",
    "print('Number of entries with label 1 (meeting planned) =', label_counts[1])\n",
    "print('Portion of entries with label 1 =', round(label_counts[1]/df.shape[0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190733a",
   "metadata": {},
   "source": [
    "The dataset should contain 1558 entries in total. If not, please double check which file you are loading. \n",
    "\n",
    "Look at that! There are many more entries with label 0 than entries with labels 1. In the previous tutorial, there were as many entries with label 0 as with label 1, remember? Later, we will discuss if this is a problem and what we can do in such a situation, but for now let's prepare our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e29724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in the previous tutorial let's clean our texts. This time we will make a function to clean texts\n",
    "# using the same code we used in the previous tutorial.\n",
    "\n",
    "def delete_substring(pattern, string): \n",
    "    return re.sub(pattern, '', str(string))\n",
    "\n",
    "def clean_text (df): \n",
    "    # Lower the text \n",
    "    df[\"clean_text\"] = df[\"text\"].str.lower()\n",
    "    # Delete [removed] from the text\n",
    "    df[\"clean_text\"] = df.apply(lambda x: delete_substring(r'\\[removed]', x[\"clean_text\"]), axis = 1)\n",
    "    # Remove anything that is not a letter, space or digit\n",
    "    df[\"clean_text\"] = df.apply(lambda x: delete_substring(r'[^\\w\\s]', x[\"clean_text\"]), axis = 1)\n",
    "    return df\n",
    "     \n",
    "df = clean_text(df)\n",
    "\n",
    "# Print the first 10 rows of the dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491924c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next step is to split our dataset into training and test set.\n",
    "# we use 80% of our entries for our training set and the remaining 20% for the test set\n",
    "# we set random_state to a number for reproducibility. In this way, every time you run this cell, the split of the data\n",
    "# will be exactly the same\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=4)\n",
    "print('Training set:')\n",
    "print('(number of rows, number of columns) =', df_train.shape)\n",
    "print('Number of entries with label 0 =', Counter(df_train['label'])[0])\n",
    "print('Number of entries with label 1 =', Counter(df_train['label'])[1])\n",
    "print('Portion of entries with label 1 =', round(Counter(df_train['label'])[1]/df_train.shape[0], 3))\n",
    "\n",
    "print('____________________________________________________')\n",
    "print('Test set:')\n",
    "print('(number of rows, number of columns) =', df_test.shape)\n",
    "print('Number of entries with label 0 =', Counter(df_test['label'])[0])\n",
    "print('Number of entries with label 1 =', Counter(df_test['label'])[1])\n",
    "print('Portion of entries with label 1 =', round(Counter(df_test['label'])[1]/df_test.shape[0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd52b8b",
   "metadata": {},
   "source": [
    "We already saw that 19.5% of our entries have label 1. Notice how close to 19.5% the portion of entries with label 1 in the training and test set is. That is thanks to `train_test_split` function that keeps the proportions of our training and test set similar to those in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's continue by creating our features\n",
    "cv = CountVectorizer()\n",
    "train_features = cv.fit_transform(df_train['clean_text'])  \n",
    "test_features = cv.transform(df_test['clean_text'])\n",
    "# QUESTION: do you remember why we call fit_transform on the training data only?\n",
    "\n",
    "# let's see the number of features after transformation\n",
    "print('Training set: (number of rows, number of columns) =', train_features.shape)\n",
    "print('Test set: (number of rows, number of columns) =', test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430154b",
   "metadata": {},
   "source": [
    "WOW! We have way more columns than rows! The model that we use can handle such a sparse dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e6da5",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "Before we continue, let's define a function that evaluates a model in terms of accuracy, recall, and confusion matrix. It is something that we will use a lot in this tutorial, so it is good to have such a function instead of copy-pasting the same code all the time (This is why we have functions!!).\n",
    "\n",
    "But first things first, what is accuracy? Recall? Confusion matrix? Are you confused already?\n",
    "\n",
    "Assume that we only have 10 entries in our dataset and that we have already trained a model to predict whether a message is about planning a meeting or not (label 1 or 0, respectively). The table below shows the first 10 entries of our dataset and the 'predicted' labels. Let's evaluate our model based on those entries.\n",
    "\n",
    "| id     | clean_text                                          | label | predicted label |\n",
    "| -----: | --------------------------------------------------: | ----: | ---:            |\n",
    "| 116725 | ja k denk dat wij wel eerder klaar zijn was zo...   |   0   |  1\n",
    "| 116726 | ik heb trouwens ook een samenvatting van dat b...   |   0   |  0\n",
    "| 116727 | ik kom speciaal voor jou supetvroeg naar het o...   |   1   |  0 \n",
    "| 116728 | maar zelfs als ik kan blijven slapen dan moet ...   |   1   |  1\n",
    "| 116729 |                ligt eraan wanneer je m nodig hebt   |   0   |  1\n",
    "| 116730 |                        wilde plannen dit weekend    |   0   |  0\n",
    "| 116731 |     lol heb al  maanden niet gestofzuigd            |   0   |  0 \n",
    "| 116732 | op de officiÃ«le site tenminste dat denk ik dat...   |   0   |  1 \n",
    "| 116733 |                              tot morgenavond he     |   1   |  0\n",
    "| 116734 |                                   waar ben je dan   |   0   |  0\n",
    "\n",
    "\n",
    "\n",
    "Its **accuracy** is just how often our model has correctly predicted the label about the meeting. As we can see, 5 out of 10 times our models predicted the correct label, so our accuracy is 5/10 = 0.5. \n",
    "\n",
    "What about its recall? Well, for recall first we have to pick which class is of importance for us. In this case, the minority class is the one that we are interested in. Then, **recall** is for how many entries with label 1 the model has also predicted 1. Back to our dummy example, we see that out of 3 entries with label 1 only once the model has provided the correct label (the model is not so great, therefore we don't recommend you to blindly assign labels). So our recall is 1/3 = 0.33.\n",
    "\n",
    "Finally, the **confusion matrix** is simply a 'summary' of the model performance. It provides per label how many entries have been classified correctly and how many not. So the confusion matrix in this case is the following:\n",
    "\n",
    "|                |             |        |\n",
    "| -------------- |-------------| ------------------------------------ |\n",
    "|                | Predicted: no meeting (0)  | Predicted: meeting (1)|\n",
    "| No meeting (0) | 4                          |   3                   |\n",
    "| Meeting (1)    | 2                          |   1                   |\n",
    "\n",
    "Now we can clearly see that 4 entries with label 0 (no meeting) were correctly predicted as 0, while 3 entries with label 0 were misclassified as 1 (meeting). Similar, we can observe that 2 entries with label 1 have given the wrong prediction by our model and 1 entry with label 1 is correctly classified. We can also use a confusion matrix to calculate the accuracy = (4+1)/(4+3+2+1) = 5/10 = 0.5 and recall = 1/(2+1) = 1/3 = 0.33.\n",
    "\n",
    "\n",
    "To complete the code in the next cell, please check the following links: \n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48969378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple function that, given predictions and the true labels of some observations, returns\n",
    "# the accuracy, recall, and confusion matrix\n",
    "def evaluate_model(ground_truth, predictions):\n",
    "    \n",
    "    # Replace None in each line below so that model accuracy, recall, and confusion matrix are calculated.\n",
    "    acc = None  # accuracy\n",
    "    recall = None  # recall\n",
    "    con_ma = None  # confusion matrix\n",
    "    \n",
    "    return acc, recall, con_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to test your function\n",
    "toy_predictions = [0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0]\n",
    "toy_labels = [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0]\n",
    "toy_accuracy, toy_recall, toy_conf_matrix = evaluate_model(toy_labels, toy_predictions)\n",
    "print('accuracy =', toy_accuracy)\n",
    "print('recall =', round(toy_recall, 3))\n",
    "print('Confusion matrix =', toy_conf_matrix.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95a092",
   "metadata": {},
   "source": [
    "If your accuracy is equal to 0.6, good job! If recall is equal to 0.615, amazing! Finally, if the confusion matrix is [[10, 7],[5, 8]] then wonderful! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cc2e4",
   "metadata": {},
   "source": [
    "## Imbalanced data\n",
    "\n",
    "You are dealing with imbalanced data when the frequency one group of entries is much higher than the frequency of another group. In our case, there are 1254 entries with label 0 and only 304 entries with label 1. We refer to the group of entries with label 0 as our majority class and to the group of entries with label 1 as our minority class.\n",
    "\n",
    "So, why is this a problem? Let's follow the steps of the previous tutorial and closely inspect the performance of our model. This time we will fit a support vector machine model on our data (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). This model is a popular classification model (like logistic regression), but is better suited for sparse datasets. See for more information [*the Wikipedia page*](https://en.wikipedia.org/wiki/Support_vector_machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first initialize a support vector machine model with the default configuration.\n",
    "base_model = SVC()\n",
    "\n",
    "# Replace None below with the command for fitting the base_model on train_features\n",
    "None  \n",
    "\n",
    "# Derive predictions on our test set. \n",
    "base_predictions = base_model.predict(test_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d7186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check the accuracy of our model\n",
    "base_accuracy, base_recall, base_conf_matrix = evaluate_model(df_test['label'], base_predictions)\n",
    "base_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e875605",
   "metadata": {},
   "source": [
    "Oh nice, almost 81%! Not bad at all, right? But what about our recall? When the text is actually about planning a meeting, how often did our model predict the correct label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21417612",
   "metadata": {},
   "source": [
    "Oh that is not nice at all! So from all the entries with label 1 in our test set, only 3.2% of them was correctly classified. Let's inspect the confusion matrix to see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run the cell. The code below is just to make the confusion matrix easier to read.\n",
    "cmd = ConfusionMatrixDisplay(base_conf_matrix)\n",
    "cmd.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8f347",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "So, what can we do about this? Below, we briefly discuss a few ways of working with imbalanced data:\n",
    "\n",
    " - Handling imbalanced data on a data-level: For example, oversampling the minority class (e.g. create new entries by duplication) or undersampling the majority class (e.g. remove entries). Be mindful of the shortcomings of those techniques. By undersampling your majority class, valuable information may get lost and by oversampling your minority class, your model might end up 'recognizing' those instances so well that it isn't able to generalize to other instances that also are part of the minority class.\n",
    "\n",
    " - Handling imbalanced data on an algorithm-level: Carefully choose the parameters of the model. For example, logistic regression and support vector machine in Python have a parameter called 'class_weight' that you can set to 'balanced'. When you do that, mistakes made in the minority class during training are penalized more than mistakes in the majority class. Therefore, the models put an effort in correctly classifying the minority class.  \n",
    "\n",
    " - Of course, one can always use a combination of the aforementioned approaches. \n",
    " \n",
    " \n",
    " Here, we only covered few approaches, feel free to search online for other ways to handle imbalanced data. And remember that each case is unique, a method that worked well for one situation might not be the best for another.\n",
    " \n",
    " In this tutorial, we will set the parameter class_weight to 'balanced' when setting up our support vector machine model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29876952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize a new support vector machine with the class_weight set to 'balanced'.\n",
    "\n",
    "model = None\n",
    "\n",
    "model.fit(train_features, df_train['label'])\n",
    "predictions = model.predict(test_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the performance of this model\n",
    "accuracy, recall, conf_matrix = evaluate_model(df_test['label'], predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1e82a",
   "metadata": {},
   "source": [
    "Em that is sighty less than before. Let's see our recall and confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38577c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall = ', recall)\n",
    "print('Confusion Matrix:')\n",
    "cmd = ConfusionMatrixDisplay(conf_matrix)\n",
    "cmd.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a870f",
   "metadata": {},
   "source": [
    "Oh, we went from 0.03 to 0.5. That's quite an improvement. So, even though we lost a bit on the overall performance, we gain quite a lot on correctly classifying the minority class. (Still of course, there is quite some space to further improve this model!)\n",
    "\n",
    "By the way, in the previous tutorial (wg3_intro_to_ML), we actually undersampled the majority class for you by randomly removing entries with label 0. Which approach is better for this dataset? Can you think why? \n",
    "\n",
    "Note that in the previous turorial, we used logistic regression as our model. If you want to directly compare the undersampling method to the usage of the 'class_weight' parameter, maybe it would have been nice if we had also used a logistic regression model in this tutorial. So, feel free to go and replace 'SVC' model with 'LogisticRegression' (who knows, you might even get better results)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14080884",
   "metadata": {},
   "source": [
    "## Training vs test set\n",
    "\n",
    "One of the most important steps is to split our dataset to training and test set, but why? In some cases, we have to split it into more parts but this is a story for another day. Let's see what happens when we evaluate our model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32042f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics on the training set\n",
    "# Replace None with the appropriate line of code\n",
    "train_predictions = None  # derive prediction using train_features\n",
    "\n",
    "# derive predictions on our training set\n",
    "train_accuracy, train_recall, train_conf_matrix = evaluate_model(df_train['label'], train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "print('Accuracy =', train_accuracy)\n",
    "print('Recall =', train_recall)\n",
    "\n",
    "print('_______________________________')\n",
    "print('Test set:')\n",
    "print('Accuracy =', accuracy)\n",
    "print('Recall =', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1106f8",
   "metadata": {},
   "source": [
    "What do you observe? \n",
    "\n",
    "\n",
    "Our train accuracy and recall are higher than test accuracy and recall. A machine learning model will almost always achieve much better performance on the data that it has already seen. That is why we split the dataset into two parts, we train our model on one part and evaluate its performance on the other part. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e843ac6",
   "metadata": {},
   "source": [
    "## Bias or variance? High or low?\n",
    "\n",
    "What conclusions can be drawn from the difference in performance between training and test set? One can argue that we are dealing with overfitting (high variance), and indeed that will be the case. This means that our models 'learned' so well our training data that it is not able to generalize well and proper classify unseen data. You are dealing with this kind of situations when the difference between the performance on the training set and test set is so high. \n",
    "\n",
    "For our case, this can be due to class imbalance. We could consider trying other methods or a combination of methods to better handle it. Or it might be because we are using way too many features and so, we have to figure out how to reduce the dimension of the data without losing information. Finally, we could go back to our model and 'play' with parameters, that helps with the regularization of the model (and so with overfitting). Sometimes, it helps to just simplify the model. There are several things that one can try. \n",
    "\n",
    "Let's try playing with the regularization parameter of the SVC model. For SVC, the regularization parameter is called C. \n",
    "\n",
    "What is the default value for the regularization parameter C? You can find the answer in the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html As we have not specified the parameter, this is the value that we have been using until now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ee52a-0476-42eb-a8dc-6eee5efd14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to initialize new SVC models with higher and lower regularization than the default value to see the effect on overfitting\n",
    "\n",
    "# Let's make a function for this \n",
    "# Replace None with the appropriate code to initialize the SVC model with the regularization parameter C\n",
    "\n",
    "def create_regularized_SVC(C): \n",
    "    model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2fd9c-5418-42d2-9d89-e496233ba39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function to fit and evaluate your new SVC model. We have put all the functions for fitting, evaluating and \n",
    "# printing the metrics in one function to make the code easier to reuse. \n",
    "\n",
    "def fit_and_evaluate_SVC(model):\n",
    "    model.fit(train_features, df_train['label'])\n",
    "    predictions = model.predict(test_features)\n",
    "    accuracy, recall, conf_matrix = evaluate_model(df_test['label'], predictions)\n",
    "    \n",
    "    train_predictions = model.predict(train_features) \n",
    "    train_accuracy, train_recall, train_conf_matrix = evaluate_model(df_train['label'], train_predictions)\n",
    "    \n",
    "    print('_______________________________')\n",
    "    print('Training set:')\n",
    "    print('Accuracy =', train_accuracy)\n",
    "    print('Recall =', train_recall)\n",
    "\n",
    "    print('_______________________________')\n",
    "    print('Test set:')\n",
    "    print('Accuracy =', accuracy)\n",
    "    print('Recall =', recall)\n",
    "    \n",
    "# Initialize a SVC model with low regularization (a high value for C is low regularization for SVC models) by setting C to 100\n",
    "# make use of the create_regularized_SVC function you have created.\n",
    "# Replace None with the appropriate code.\n",
    "\n",
    "model = None\n",
    "\n",
    "# We will fit and evaluate the model \n",
    "fit_and_evaluate_SVC(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7005e8a-8bf6-48d5-b685-07c9c38c7c72",
   "metadata": {},
   "source": [
    "So with lower regularization the overfitting has gotten worse. Look at the accuracy and recall on the training data! It is above 99%! The accuracy on the test data is also a bit higher than before but the recall is much lower. The model now learns on the training data so well that it is not able to generalize to unseen data such as the data in the test set.\n",
    "\n",
    "Let's try the opposite: We will make the regularization stronger by lowering the value for C below the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bf669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SVC model with higher regularization value than the default (the lower the value for C, the stronger the regularization). \n",
    "# Create a SVC model with C set to 0.5 by replacing the None with the appropriate code  \n",
    "\n",
    "model = None\n",
    "\n",
    "# We will fit and evaluate the model \n",
    "fit_and_evaluate_SVC(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe492e-bcd1-4ec0-92ec-e5ea8d21676b",
   "metadata": {},
   "source": [
    "Increasing the regularization has reduced overfitting as the outcomes for the training data and the test data are now similar (both around 0.8). This means that the model generalizes very well. \n",
    "\n",
    "However this does not mean that the model is good. Its recall is terrible! In fact, the recall is zero. What does that mean? Why do you think the accuracy is still around 80 % ?\n",
    "\n",
    "We can also combine multiple methods. What if we increase the regularization as well as setting the class_weight parameter to `balanced` in our SVC model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84cf75-0ca1-49fe-a554-e293f29b9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function for creating a SVC model that accepts a regularization parameter as input and has the class_weight set to `balanced'\n",
    "# Replace None with the appropriate code\n",
    "\n",
    "def create_regularized_SVC_balanced(C): \n",
    "    model = None\n",
    "    return model\n",
    "\n",
    "# Create a SVC model with the regularization parameter (C) set to 0.5 and class_weight = 'balanced' \n",
    "\n",
    "model = create_regularized_SVC_balanced(0.5)\n",
    "\n",
    "# We will fit and evaluate the model \n",
    "fit_and_evaluate_SVC(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fcfbc-769c-4d4c-b530-07d7e846d47d",
   "metadata": {},
   "source": [
    "That looks better! The higher performance on the test set means that the model is able to generalize better. In other words, this model works better on data that is has not seen before.\n",
    "\n",
    "Have fun and play around with the regularization parameter to see what happens and if you can improve the model even more. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac6937",
   "metadata": {},
   "source": [
    "## Data mismatch\n",
    "\n",
    "So far we have used WhatApp messages in Dutch to train and evaluate our model (and we have just discussed why we need to split our dataset to training and test set). Now that we have a trained model and we know how well it performs, can we apply it on any kind of text?\n",
    "\n",
    "If your answer is 'no, because our model still needs some work to improve its performance', assume that it performs great on training and test set. What about now? Can we use it on any text that we are interested in? Let's start with the basics, can we use it on English text? What if the text is in Dutch but it's an email? Or a transcription of a telephone conversation? What do you expect to occur in these situations?\n",
    "\n",
    "To get a feeling of what to expect in these situations, let's use our model on few instances that are no part of the dataset with the WhatsApp messages. We will use data from the Instagram page @liefdevantoen. It contains personal ads from newspapers from between 1840 and 1940, some of which discussing meetings. Let's see how our model classifies those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fd8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the new dataset\n",
    "new_file = 'wg4_doing_ml_right_cross_domain_data.csv'\n",
    "cross_domain_df = pd.read_csv(new_file)\n",
    "cross_domain_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529eb50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many entries in total and per label are there?\n",
    "print('Number of entries = ', cross_domain_df.shape[0])\n",
    "print('Number of entries with label 0 (no meeting) =', Counter(cross_domain_df['label'])[0])\n",
    "print('Number of entries with label 1 (meeting planned) =', Counter(cross_domain_df['label'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare our dataset the same way as the WhatsApp data using the clean_text function we made\n",
    "\n",
    "cross_domain_df = clean_text(cross_domain_df)\n",
    "\n",
    "# and let's transform our text to features\n",
    "# replace None so that we get the same features as before (remember? we fitted a CountVectorizer earlier)\n",
    "features = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ab379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace None to derive the model predictions\n",
    "cross_domain_predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02912dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the evaluation metrics\n",
    "cd_accuracy, cd_recall, cd_conf_matrix = evaluate_model(cross_domain_df['label'], cross_domain_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d607c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace None to print the accuracy\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0dbfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the recall\n",
    "print('Recall =', cd_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "cmd = ConfusionMatrixDisplay(cd_conf_matrix)\n",
    "cmd.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47e216",
   "metadata": {},
   "source": [
    "So what happened to our metrics? Why?\n",
    "\n",
    "Over time, language and spelling conventions have changed, which might make it more difficult for our model to classify the messages correctly. In such situations, one has to be mindful on how to close the gap between the data available for training and testing the model and the data we actually want to derive predictions. A thoughtful choice of features might be quite helpful here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e115d5a-f60c-467a-8047-7c8997109452",
   "metadata": {},
   "source": [
    "# Further (optional) exercises\n",
    "\n",
    "1. You learned about cross-validation today. In this workbook we have only used one train-test split. You can now experiment with cross-validation. Have a look at this cross_val_score function:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score. By default it does a 5-fold cross-validation. You can also try to implement a 10-fold cross-validation instead by changing the cv parameter. \n",
    "\n",
    "2. You also learned about oversampling as a solution for imbalanced data. A good package for this is imbalanced-learn (shortened: imblearn). By running the cell below you can install and import that package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36daa44-e6ef-49ef-ab10-f8aae8d0ec09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262662f3-e2e9-428f-a01f-543075c2343c",
   "metadata": {},
   "source": [
    "Now you can experiment with both random oversampling (RandomOverSampler) and SMOTE (SMOTE) on the training set. You will use train_features and df_train['label'] . Have a look at how it changes the label counts using the Counter() function.\n",
    "\n",
    "Also see these links: \n",
    "\n",
    "\n",
    "https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\n",
    "https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html?highlight=smote#imblearn.over_sampling.SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c5a8103c63b54",
   "metadata": {},
   "source": [
    "# Kaggle Challenge\n",
    "\n",
    "Do you want to put your data science skills to the test? See if you can implement your own model that predicts poisonous mushrooms in the Kaggle challenge: https://www.kaggle.com/t/3fb3213893214f28825b0f8848e471c9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
